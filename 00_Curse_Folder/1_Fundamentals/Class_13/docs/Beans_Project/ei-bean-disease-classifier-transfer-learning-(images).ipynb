{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Download the data - after extracting features through a processing block - so we can train a machine learning model."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": null,
            "source": [
                "import numpy as np\n",
                "import requests\n",
                "\n",
                "API_KEY = 'ei_fc8e4b34770a976eb34d4449fe35de18561d61be5ad968e262f66b6944a26298'\n",
                "\n",
                "def download_data(url):\n",
                "    response = requests.get(url, headers={'x-api-key': API_KEY})\n",
                "    if response.status_code == 200:\n",
                "        return response.content\n",
                "    else:\n",
                "        print(response.content)\n",
                "        raise ConnectionError('Could not download data file')\n",
                "\n",
                "X = download_data('https://studio.edgeimpulse.com/v1/api/51151/training/21/x')\n",
                "Y = download_data('https://studio.edgeimpulse.com/v1/api/51151/training/21/y')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Store the data in a temporary file, and load it back through Numpy."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": null,
            "source": [
                "with open('x_train.npy', 'wb') as file:\n",
                "    file.write(X)\n",
                "with open('y_train.npy', 'wb') as file:\n",
                "    file.write(Y)\n",
                "X = np.load('x_train.npy')\n",
                "Y = np.load('y_train.npy')[:,0]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Define our labels and split the data up in a test and training set:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": null,
            "source": [
                "import sys, os, random\n",
                "import tensorflow as tf\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "import logging\n",
                "tf.get_logger().setLevel(logging.ERROR)\n",
                "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
                "\n",
                "# Set random seeds for repeatable results\n",
                "RANDOM_SEED = 3\n",
                "random.seed(RANDOM_SEED)\n",
                "np.random.seed(RANDOM_SEED)\n",
                "tf.random.set_seed(RANDOM_SEED)\n",
                "\n",
                "classes_values = [ \"angular_leaf_spot\", \"bean_rust\", \"healthy\" ]\n",
                "classes = len(classes_values)\n",
                "\n",
                "Y = tf.keras.utils.to_categorical(Y - 1, classes)\n",
                "\n",
                "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
                "\n",
                "input_length = X_train[0].shape[0]\n",
                "\n",
                "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
                "validation_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
                "\n",
                "def get_reshape_function(reshape_to):\n",
                "    def reshape(image, label):\n",
                "        return tf.reshape(image, reshape_to), label\n",
                "    return reshape\n",
                "\n",
                "callbacks = []\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Train the model:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": null,
            "source": [
                "import math\n",
                "from pathlib import Path\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras import Model\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, InputLayer, Dropout, Conv1D, Flatten, Reshape, MaxPooling1D, BatchNormalization, Conv2D, GlobalMaxPooling2D, Lambda\n",
                "from tensorflow.keras.optimizers import Adam, Adadelta\n",
                "from tensorflow.keras.losses import categorical_crossentropy\n",
                "\n",
                "\n",
                "WEIGHTS_PATH = './transfer-learning-weights/keras/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_0.35_160.h5'\n",
                "# Download the model weights\n",
                "root_url = 'http://cdn.edgeimpulse.com/'\n",
                "p = Path(WEIGHTS_PATH)\n",
                "if not p.exists():\n",
                "    if not p.parent.exists():\n",
                "        p.parent.mkdir(parents=True)\n",
                "    weights = requests.get(root_url + WEIGHTS_PATH[2:]).content\n",
                "    with open(WEIGHTS_PATH, 'wb') as f:\n",
                "        f.write(weights)\n",
                "\n",
                "INPUT_SHAPE = (160, 160, 3)\n",
                "\n",
                "base_model = tf.keras.applications.MobileNetV2(\n",
                "    input_shape = INPUT_SHAPE, alpha=0.35,\n",
                "    weights = WEIGHTS_PATH\n",
                ")\n",
                "\n",
                "base_model.trainable = False\n",
                "\n",
                "model = Sequential()\n",
                "model.add(InputLayer(input_shape=INPUT_SHAPE, name='x_input'))\n",
                "# Don't include the base model's top layers\n",
                "last_layer_index = -3\n",
                "model.add(Model(inputs=base_model.inputs, outputs=base_model.layers[last_layer_index].output))\n",
                "model.add(Dense(16, activation='relu'))\n",
                "model.add(Dropout(0.25))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(classes, activation='softmax'))\n",
                "\n",
                "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
                "                loss='categorical_crossentropy',\n",
                "                metrics=['accuracy'])\n",
                "\n",
                "\n",
                "# Set the data to the expected input shape\n",
                "train_dataset = train_dataset.map(get_reshape_function(INPUT_SHAPE), tf.data.experimental.AUTOTUNE)\n",
                "validation_dataset = validation_dataset.map(get_reshape_function(INPUT_SHAPE), tf.data.experimental.AUTOTUNE)\n",
                "\n",
                "# Implements the data augmentation policy\n",
                "def augment_image(image, label):\n",
                "    # Flips the image randomly\n",
                "    image = tf.image.random_flip_left_right(image)\n",
                "\n",
                "    # Increase the image size, then randomly crop it down to\n",
                "    # the original dimensions\n",
                "    resize_factor = random.uniform(1, 1.2)\n",
                "    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n",
                "    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n",
                "    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n",
                "    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n",
                "\n",
                "    # Vary the brightness of the image\n",
                "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
                "\n",
                "    return image, label\n",
                "\n",
                "train_dataset = train_dataset.map(augment_image, tf.data.experimental.AUTOTUNE)\n",
                "\n",
                "BATCH_SIZE = 32\n",
                "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
                "validation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
                "\n",
                "model.fit(train_dataset, validation_data=validation_dataset, epochs=20, verbose=2, callbacks=callbacks)\n",
                "\n",
                "print('')\n",
                "print('Initial training done.', flush=True)\n",
                "\n",
                "# How many epochs we will fine tune the model\n",
                "FINE_TUNE_EPOCHS = 10\n",
                "# What percentage of the base model's layers we will fine tune\n",
                "FINE_TUNE_PERCENTAGE = 65\n",
                "\n",
                "print('Fine-tuning model for {} epochs...'.format(FINE_TUNE_EPOCHS), flush=True)\n",
                "\n",
                "# Determine which layer to begin fine tuning at\n",
                "model_layer_count = len(model.layers)\n",
                "fine_tune_from = math.ceil(model_layer_count * ((100 - FINE_TUNE_PERCENTAGE) / 100))\n",
                "\n",
                "# Allow the entire base model to be trained\n",
                "model.trainable = True\n",
                "# Freeze all the layers before the 'fine_tune_from' layer\n",
                "for layer in model.layers[:fine_tune_from]:\n",
                "    layer.trainable = False\n",
                "\n",
                "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.000045),\n",
                "                loss='categorical_crossentropy',\n",
                "                metrics=['accuracy'])\n",
                "\n",
                "model.fit(train_dataset,\n",
                "                epochs=FINE_TUNE_EPOCHS,\n",
                "                verbose=2,\n",
                "                validation_data=validation_dataset,\n",
                "                callbacks=callbacks)\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": null,
            "source": [
                "# Save the model to disk\n",
                "model.save('saved_model')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}