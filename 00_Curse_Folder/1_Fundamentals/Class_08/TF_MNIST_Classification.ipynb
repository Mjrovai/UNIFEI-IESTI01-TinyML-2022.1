{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo2JiEtkzvQL"
   },
   "source": [
    "# Digit Classification using Dense Neural Network (DNN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z56YN3JZ0Duh"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YfPnR74Gzc_q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrGLDV6q0PCc"
   },
   "source": [
    "## Upload and Explore Dataset\n",
    "[MNIST](http://yann.lecun.com/exdb/mnist/) handwritten digits dataset\n",
    "\n",
    "The MNIST database of handwritten digits, available from this [page](http://yann.lecun.com/exdb/mnist/), has a training set of 60,000 28x28 grayscale images of the 10 digits along a test set of 10,000 images. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ib2Ihepx0N6h"
   },
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.mnist\n",
    "\n",
    "(training_images, training_labels), (val_images, val_labels) = data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pq1bMIRd38b8"
   },
   "outputs": [],
   "source": [
    "print(training_images.shape)\n",
    "print(training_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHaUYnSs4bq4"
   },
   "outputs": [],
   "source": [
    "print(val_images.shape)\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5b0wydUaU1N"
   },
   "source": [
    "### Exploring Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyfWBrrW9gAd"
   },
   "outputs": [],
   "source": [
    "training_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cv8z_QGpYoEj"
   },
   "source": [
    "It is possible to keep training labels as \"numbers\", but in this case when compiling the model, you should use: `loss=\"sparse_categorical_crossentropy\".`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr8u2BMUZbwq"
   },
   "source": [
    "**And how about changing labels to categorical?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qc-FKEkWYRXL"
   },
   "outputs": [],
   "source": [
    "training_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRwje9DsX2EA"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "to_categorical(training_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cPQOQsnZtcK"
   },
   "source": [
    "When labels are defined as categories, when compiling the model you should use: `loss=\"categorical_crossentropy\".`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF4d2rJfaL3T"
   },
   "source": [
    "### Exploring images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMu0y_Az4hRr"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=200)\n",
    "print(training_images[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxbAHxFY6JVK",
    "outputId": "d78e5c94-b78a-4f4e-ef2c-c903b220e41a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3K1XrD3a5cU0"
   },
   "outputs": [],
   "source": [
    "img = 2\n",
    "print(\"     Label of image {} is: {}\".format(img, training_labels[img]))\n",
    "plt.imshow(training_images[img]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHbD0jQj7NaL"
   },
   "outputs": [],
   "source": [
    "training_images.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSY9uSFD7WOc"
   },
   "outputs": [],
   "source": [
    "training_images.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-H0LwB_7von"
   },
   "source": [
    "### Preprocessing Data \n",
    "\n",
    "**Normalizing Data**: \n",
    "We notice that all of the values in the number are between 0 and 255. If we are training a neural network, for various reasons it's easier if we treat all values as between 0 and 1, a process called 'normalizing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "uZrGVMsH0X3F"
   },
   "outputs": [],
   "source": [
    "training_images  = training_images / 255.0\n",
    "val_images = val_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Khec4VhU8mkR"
   },
   "outputs": [],
   "source": [
    "print(training_images[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbxuCDhg-djD"
   },
   "outputs": [],
   "source": [
    "training_images.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8aRT6RL8fIG"
   },
   "outputs": [],
   "source": [
    "plt.imshow(training_images[2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYMsyOk8-xnf"
   },
   "source": [
    "## Define and Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEdoqWl28dB3"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "                                    tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NA21O9Y0_BJ7"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy', # Labels are not as an array \n",
    "    metrics=['accuracy'] # Calculates how often predictions equal labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJZ4fvT3_yd7"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxklrX_R_uvn"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(\n",
    "    training_images, \n",
    "    training_labels, \n",
    "    epochs=20, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xAXSo5-HCIo"
   },
   "source": [
    "Inspecting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChxfmUKr8xrA"
   },
   "outputs": [],
   "source": [
    "train_eval = model.evaluate(training_images, training_labels)\n",
    "print (\"Training data Accuracy: {:.1f}%\".format(train_eval[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6qHpwg8BpZi"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRvANX7cHU2n"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuraccy'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYHTdEJVH0tZ"
   },
   "source": [
    "## Testing the trained model\n",
    "\n",
    "Using `model.evaluate`, you can get metrics for a test set. In this case we only have a training set and a validation set, so we can try it out with the validation set. The accuracy will be slightly lower, at maybe 96%. This is because the model hasn't previously seen this data and may not be fully generalized for all data. Still it's a pretty good score.\n",
    "You can also predict images, and compare against their actual label. The [0] image in the set is a number 7, and here you can see that neuron 7 has a 9.9e-1 (99%+) probability, so it got it right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBuJGqbSH6xT"
   },
   "outputs": [],
   "source": [
    "test_eval = model.evaluate(val_images, val_labels)\n",
    "print (\"Testing data Accuracy: {:.1f}%\".format(test_eval[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvNEv5BPIwPE"
   },
   "outputs": [],
   "source": [
    "plt.imshow(val_images[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysGu1V_PJ28-"
   },
   "outputs": [],
   "source": [
    "print(val_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkaTLMaTIbIL"
   },
   "outputs": [],
   "source": [
    "classifications = model.predict(val_images)\n",
    "print(classifications[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjenS2EZJH6h"
   },
   "outputs": [],
   "source": [
    "# Returns the indices of the maximum values along an axis.\n",
    "np.argmax(classifications[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wT3DM9IRjy6J"
   },
   "source": [
    "## Inspecting Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNzmTmJmATNQ"
   },
   "outputs": [],
   "source": [
    "predict_errors = []\n",
    "for i in range(len(val_labels)):\n",
    "  predicted_label = np.argmax(classifications[i])\n",
    "  if predicted_label != val_labels[i]:\n",
    "    predict_errors.append(i)\n",
    "\n",
    "print(1-(len(predict_errors)/len(val_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sq2oMvHyFLH_"
   },
   "outputs": [],
   "source": [
    "predict_errors[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6eEPtN5AdCX"
   },
   "outputs": [],
   "source": [
    "i = predict_errors[5]\n",
    "predicted_label = np.argmax(classifications[i])\n",
    "print(\"    Real label of image {} is: {}\".format(i, val_labels[i]))\n",
    "print(\" Predict label of image {} is: {}\".format(i, predicted_label))\n",
    "plt.imshow(val_images[i]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_D_yEaAKHlr"
   },
   "source": [
    "## Modify to inspect learned values\n",
    "\n",
    "This code is identical, except that the layers are named prior to adding to the sequential. This allows us to inspect their learned parameters later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCwgerwjKaLz"
   },
   "outputs": [],
   "source": [
    "layer_1 = tf.keras.layers.Dense(20, activation=tf.nn.relu)\n",
    "layer_2 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "                                    layer_1,\n",
    "                                    layer_2])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, \n",
    "          training_labels, \n",
    "          epochs=20,\n",
    "          verbose=0\n",
    "          )\n",
    "\n",
    "test_eval = model.evaluate(val_images, val_labels)\n",
    "print (\"Testing data Accuracy: {:.1f}%\".format(test_eval[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNUJdZ6gKwf0"
   },
   "outputs": [],
   "source": [
    "classifications = model.predict(val_images)\n",
    "print(classifications[0])\n",
    "print(np.argmax(classifications[0]))\n",
    "print(val_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr4qEvowLSai"
   },
   "source": [
    "Inspect Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qbgKY71Latz"
   },
   "outputs": [],
   "source": [
    "print(\"Layer 1 Ws ==>\", layer_1.get_weights()[0].size)\n",
    "print(\"Layer 1 bs ==>\", layer_1.get_weights()[1].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IOf5K5_LqLw"
   },
   "source": [
    "The `get_weights()[0]` contains the weights (Ws). Why we have 15,680 w's on layer 1? Recall that there are 20 neurons in the first layer and, that the images are 28x28 pixels, which is 784. So, if you multiply 784 x 20 you get 15,680.\n",
    "\n",
    "This layer has 20 neurons, and each neuron learns a W parameter for each pixel. So instead of y=Mx+c, we have \n",
    "y=M1X1+M2X2+M3X3+....+M784X784+C in every neuron! Every pixel has a weight in every neuron. Those weights are multiplied by the pixel value, summed up, and given a bias.\n",
    "\n",
    "The `get_weights()[1]` contains the biases (bs), one for each of the 20 neurons in this layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtzjbvbeNxts"
   },
   "source": [
    "Inspect Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FCiM_CEOPep"
   },
   "outputs": [],
   "source": [
    "print(\"Layer 2 Ws ==>\", layer_2.get_weights()[0].size)\n",
    "print(\"Layer 2 bs ==>\", layer_2.get_weights()[1].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbvQq_3vOlVf"
   },
   "source": [
    "Layer 2 has 200 Ws. This is because are 10 neurons in this layer, but there are 20 neurons in the previous layer. So, each neuron in this layer will learn a weight for the incoming value from the previous layer. So, for example, the if the first neuron in this layer is N21, and the neurons output from the previous layers are N11-N120, then this neuron will have 20 weights (W1-W20) and it will calculate its output to be:\n",
    "\n",
    "`W1N11+W2N12+W3N13+...+W20N120+Bias`\n",
    "\n",
    "So each of these weights will be learned as will the bias, for every neuron.\n",
    "\n",
    "Note that N11 refers to Layer 1 Neuron 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnPuvtrWPJ0f"
   },
   "source": [
    "...and as expected there are 10 elements in the second array, representing the 10 biases for the 10 neurons.\n",
    "Hopefully this helps you see how the element of a simple neuron containing y=mx+c can be expanded greatly into a deep neural network, and that DNN can learn the parameters that match the 784 pixels of an image to their output!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "TF_MNIST_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
